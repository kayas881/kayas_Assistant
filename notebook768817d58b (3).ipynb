{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kayas Assistant ‚Äî QLoRA Fine-tuning on Kaggle (T4x2 FSDP)\n",
    "\n",
    "**Model:** `Qwen/Qwen2.5-7B-Instruct`\n",
    "**Hardware:** `T4 x2` (2x 16GB GPUs)\n",
    "**Strategy:** FSDP + QLoRA + Flash Attention 2 for maximum speed and memory efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Environment check\n",
    "import os, sys, platform, shutil\n",
    "print('Python:', sys.version)\n",
    "print('OS:', platform.platform())\n",
    "\n",
    "# List input data directory\n",
    "INPUT_DIR = '/kaggle/input'\n",
    "if os.path.exists(INPUT_DIR):\n",
    "    print(\"\\nInput data:\")\n",
    "    for root, dirs, files in os.walk(INPUT_DIR):\n",
    "        for name in files:\n",
    "            print(f\"- {os.path.join(root, name)}\")\n",
    "else:\n",
    "    print(f\"\\nWarning: Input directory '{INPUT_DIR}' not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- 1. Install Dependencies (bnb + triton last) ---\n",
    "print(\"üöÄ Installing dependencies...\")\n",
    "print(\"   This part is silent and will take 1-2 minutes...\")\n",
    "\n",
    "# Install main libraries first\n",
    "!pip install -q \\\n",
    "    \"torch==2.3.1+cu121\" \\\n",
    "    \"transformers==4.43.3\" \\\n",
    "    \"peft==0.12.0\" \\\n",
    "    \"trl==0.9.6\" \\\n",
    "    \"datasets==2.20.0\" \\\n",
    "    \"accelerate==0.32.1\" \\\n",
    "    \"pyarrow==22.0.0\" \\\n",
    "    \"rich\" \\\n",
    "    -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "# Install bitsandbytes and triton (for quantization / LoRA)\n",
    "!pip install -q bitsandbytes==0.43.1 triton==2.3.0\n",
    "\n",
    "print(\"‚úÖ All dependencies installed successfully.\")\n",
    "\n",
    "# --- 2. RESTART KERNEL ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üõë IMPORTANT: YOU MUST RESTART THE KERNEL NOW!\")\n",
    "print(\"Go to 'Run' > 'Restart Session' in the Kaggle menu.\")\n",
    "print(\"Do NOT run this cell again after restarting.\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-25T12:01:02.839811Z",
     "iopub.status.busy": "2025-10-25T12:01:02.839090Z",
     "iopub.status.idle": "2025-10-25T12:01:02.844003Z",
     "shell.execute_reply": "2025-10-25T12:01:02.843270Z",
     "shell.execute_reply.started": "2025-10-25T12:01:02.839784Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    GenerationConfig\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-25T12:01:08.696378Z",
     "iopub.status.busy": "2025-10-25T12:01:08.695782Z",
     "iopub.status.idle": "2025-10-25T12:01:08.713543Z",
     "shell.execute_reply": "2025-10-25T12:01:08.712984Z",
     "shell.execute_reply.started": "2025-10-25T12:01:08.696354Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found dataset at: /kaggle/input/kayass/mega_brain_dataset_10000_enhanced.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Auto-discover the dataset path\n",
    "# Tries enhanced 10k first, then 8k, then any mega_brain_dataset_*.jsonl under your dataset folder.\n",
    "\n",
    "def find_dataset_path(base_dir='/kaggle/input'):\n",
    "    preferred = [\n",
    "        'mega_brain_dataset_10000_enhanced.jsonl',\n",
    "        'mega_brain_dataset_8000_enhanced.jsonl',\n",
    "        'mega_brain_dataset_10000.jsonl',\n",
    "        'mega_brain_dataset_8000.jsonl',\n",
    "    ]\n",
    "    # If you know your dataset subfolder, you can narrow base_dir to '/kaggle/input/kayass/'\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            if file in preferred or (file.startswith('mega_brain_dataset_') and file.endswith('.jsonl')):\n",
    "                path = os.path.join(root, file)\n",
    "                print(f\"‚úÖ Found dataset at: {path}\")\n",
    "                return path\n",
    "    print(f\"‚ùå ERROR: No mega_brain_dataset_*.jsonl found in {base_dir}\")\n",
    "    print(\"Please check your 'Add Data' settings or set DATASET_PATH manually.\")\n",
    "    return None\n",
    "\n",
    "DATASET_PATH = find_dataset_path()\n",
    "if DATASET_PATH is None:\n",
    "    raise FileNotFoundError(\"Could not find the dataset file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-25T12:16:42.926678Z",
     "iopub.status.busy": "2025-10-25T12:16:42.926414Z",
     "iopub.status.idle": "2025-10-25T12:16:42.933025Z",
     "shell.execute_reply": "2025-10-25T12:16:42.932170Z",
     "shell.execute_reply.started": "2025-10-25T12:16:42.926660Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Qwen/Qwen2.5-7B-Instruct\n",
      "Output: /kaggle/working/brain-lora-7b\n",
      "Seq Len: 1024\n",
      "Effective Batch Size: 16 (over 2 GPUs)\n"
     ]
    }
   ],
   "source": [
    "# --- Parameters ---\n",
    "\n",
    "# Model\n",
    "BASE_MODEL = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "ALLOW_MODEL_FALLBACK = True  # If OOM at load, retry with 3B automatically\n",
    "\n",
    "# Dataset\n",
    "# DATASET_PATH is set from Cell 5\n",
    "\n",
    "# Output\n",
    "OUTPUT_DIR = Path(\"/kaggle/working/brain-lora-7b\")\n",
    "\n",
    "# QLoRA\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.05\n",
    "LORA_TARGET_MODULES = [\n",
    "    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "    \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "]\n",
    "\n",
    "# Training\n",
    "MAX_SEQ_LEN = 1024 # Keep at 1024\n",
    "BATCH_SIZE = 1     # Keep at 1 per device\n",
    "GRAD_ACCUM_STEPS = 8 # Reduced for stability on T4\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 2e-4\n",
    "MAX_GRAD_NORM = 0.3\n",
    "WARMUP_RATIO = 0.03\n",
    "\n",
    "# Memory/loader knobs\n",
    "DISABLE_FLASH_ATTENTION = True  # Force eager attention to reduce peak VRAM\n",
    "\n",
    "# --- End Parameters ---\n",
    "\n",
    "# Create output dir\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "print(f\"Model: {BASE_MODEL}\")\n",
    "print(f\"Output: {OUTPUT_DIR}\")\n",
    "print(f\"Seq Len: {MAX_SEQ_LEN}\")\n",
    "print(f\"Effective Batch Size: {BATCH_SIZE * GRAD_ACCUM_STEPS * max(1, torch.cuda.device_count())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-25T12:16:45.574489Z",
     "iopub.status.busy": "2025-10-25T12:16:45.574212Z",
     "iopub.status.idle": "2025-10-25T12:16:46.383241Z",
     "shell.execute_reply": "2025-10-25T12:16:46.382388Z",
     "shell.execute_reply.started": "2025-10-25T12:16:45.574469Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from /kaggle/input/kayass/mega_brain_dataset_10000_enhanced.jsonl...\n",
      "Applying chat template to dataset...\n",
      "‚úÖ Chat template applied.\n",
      "\n",
      "‚úÖ Dataset loaded, formatted, and split:\n",
      "   Train samples: 9500\n",
      "   Validation samples: 500\n",
      "\n",
      "   Example formatted text:\n",
      "<|im_start|>system\n",
      "You are Kayas, a friendly and helpful AI assistant. You have a warm personality and genuinely care about helping users.\n",
      "\n",
      "When users ask you to do something:\n",
      "1. First, acknowledge their request warmly\n",
      "2. Then provide the JSON tool calls\n",
      "3. Optionally add a brief confirmation\n",
      "\n",
      "Format:\n",
      "{\n",
      "  \"response\": \"Your friendly message here\",\n",
      "  \"actions\": [{\"tool\": \"...\", \"args\": {...}}]\n",
      "}\n",
      "\n",
      "Be natural, friendly, and professional. Use emojis sparingly and appropriately.<|im_end|>\n",
      "<|im_start|>...\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "print(f\"Loading dataset from {DATASET_PATH}...\")\n",
    "dataset = load_dataset(\"json\", data_files=DATASET_PATH, split=\"train\")\n",
    "\n",
    "# --- NEW: Manually Apply Chat Template ---\n",
    "print(\"Applying chat template to dataset...\")\n",
    "# Load tokenizer temporarily just for applying the template\n",
    "tokenizer_for_template = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True, trust_remote_code=True)\n",
    "if tokenizer_for_template.pad_token_id is None:\n",
    "    tokenizer_for_template.pad_token = tokenizer_for_template.eos_token\n",
    "\n",
    "def apply_template(example):\n",
    "    # Takes the 'messages' list and converts it to a single formatted string\n",
    "    return {\"formatted_text\": tokenizer_for_template.apply_chat_template(\n",
    "        example[\"messages\"], \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=False # Important for training!\n",
    "    )}\n",
    "\n",
    "# Apply the function to the dataset\n",
    "dataset = dataset.map(apply_template, num_proc=os.cpu_count()) \n",
    "# Remove the original messages column as it's no longer needed by SFTTrainer\n",
    "# dataset = dataset.remove_columns(\"messages\") \n",
    "# ^^^ Keep messages for now, might be useful if SFTTrainer still needs it internally\n",
    "\n",
    "print(\"‚úÖ Chat template applied.\")\n",
    "# --- End NEW section ---\n",
    "\n",
    "\n",
    "# Split the dataset\n",
    "train_val_split = dataset.train_test_split(test_size=0.05, seed=42) # 5% for validation\n",
    "train_data = train_val_split[\"train\"]\n",
    "val_data = train_val_split[\"test\"]\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset loaded, formatted, and split:\")\n",
    "print(f\"   Train samples: {len(train_data)}\")\n",
    "print(f\"   Validation samples: {len(val_data)}\")\n",
    "# Optional: Print first example's formatted text\n",
    "if len(train_data) > 0:\n",
    "    print(\"\\n   Example formatted text:\")\n",
    "    print(train_data[0]['formatted_text'][:500] + \"...\") # Show first 500 chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-25T12:16:49.726189Z",
     "iopub.status.busy": "2025-10-25T12:16:49.725600Z",
     "iopub.status.idle": "2025-10-25T12:16:49.735302Z",
     "shell.execute_reply": "2025-10-25T12:16:49.734493Z",
     "shell.execute_reply.started": "2025-10-25T12:16:49.726164Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training function defined.\n"
     ]
    }
   ],
   "source": [
    "def main_training_function():\n",
    "    # --- IMPORTS MOVED INSIDE ---\n",
    "    import torch\n",
    "    import torch.distributed as dist\n",
    "    from datasets import load_dataset # Keep this import if you reload inside\n",
    "    from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "    from transformers import (\n",
    "        AutoModelForCausalLM,\n",
    "        AutoTokenizer,\n",
    "        BitsAndBytesConfig,\n",
    "        TrainingArguments,\n",
    "        GenerationConfig\n",
    "    )\n",
    "    from trl import SFTTrainer\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "\n",
    "    # --- Device / env setup per process ---\n",
    "    local_rank = int(os.environ.get('LOCAL_RANK', os.environ.get('RANK', 0)))\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.set_device(local_rank)\n",
    "    os.environ.setdefault('PYTORCH_CUDA_ALLOC_CONF', 'expandable_segments:True')\n",
    "\n",
    "    # --- 1. Load Tokenizer ---\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True, trust_remote_code=True)\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # --- 2. QLoRA Config ---\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "\n",
    "    def _load_base(model_name: str):\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        print(f\"Process {os.environ.get('RANK', 0)}: Loading base model {model_name}...\")\n",
    "        mdl = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map={\"\": local_rank} if torch.cuda.is_available() else None,\n",
    "            torch_dtype=torch.float16,\n",
    "            trust_remote_code=True,\n",
    "            low_cpu_mem_usage=True,\n",
    "        )\n",
    "        # Lower peak memory by disabling flash attention and cache\n",
    "        try:\n",
    "            if DISABLE_FLASH_ATTENTION:\n",
    "                mdl.config.attn_implementation = 'eager'\n",
    "        except Exception:\n",
    "            pass\n",
    "        mdl.config.use_cache = False\n",
    "        print(f\"Process {os.environ.get('RANK', 0)}: Base model loaded.\")\n",
    "        return mdl\n",
    "\n",
    "    # --- 3. Load Model with fallback on OOM ---\n",
    "    try:\n",
    "        model = _load_base(BASE_MODEL)\n",
    "    except torch.cuda.OutOfMemoryError as e:\n",
    "        if ALLOW_MODEL_FALLBACK:\n",
    "            print(\"\\n‚ö†Ô∏è OOM at load with 7B. Falling back to Qwen/Qwen2.5-3B-Instruct...\")\n",
    "            fallback = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "            global BASE_MODEL\n",
    "            BASE_MODEL = fallback\n",
    "            tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True, trust_remote_code=True)\n",
    "            if tokenizer.pad_token_id is None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "            model = _load_base(BASE_MODEL)\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "    # --- 4. LoRA Config ---\n",
    "    lora_config = LoraConfig(\n",
    "        r=LORA_R,\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        lora_dropout=LORA_DROPOUT,\n",
    "        target_modules=LORA_TARGET_MODULES,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "    # --- 5. Training Arguments (with FSDP) ---\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=str(OUTPUT_DIR),\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=200,\n",
    "        save_total_limit=2,\n",
    "        report_to=\"none\",\n",
    "        fp16=True,\n",
    "        gradient_checkpointing=False,  # FSDP activation checkpointing will be used\n",
    "        max_grad_norm=MAX_GRAD_NORM,\n",
    "        warmup_ratio=WARMUP_RATIO,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        fsdp=\"full_shard auto_wrap\",\n",
    "        fsdp_config={\n",
    "            \"transformer_layer_cls_to_wrap\": [\"Qwen2DecoderLayer\"],\n",
    "            \"activation_checkpointing\": True\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # --- 6. SFTTrainer ---\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=val_data,\n",
    "        dataset_text_field=\"formatted_text\",  # Use the pre-formatted text column\n",
    "        peft_config=lora_config,\n",
    "        packing=False,  # Keep packing disabled\n",
    "        max_seq_length=MAX_SEQ_LEN,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    # --- 7. Start Training ---\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Process {os.environ.get('RANK', 0)}: üöÄ STARTING FSDP TRAINING...\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # --- 8. Save Final Model (Only Rank 0 saves) ---\n",
    "    print(f\"Process {os.environ.get('RANK', 0)}: ‚úÖ Training complete.\")\n",
    "    if trainer.is_world_process_zero():\n",
    "        print(\"Rank 0: Saving final adapter...\")\n",
    "        trainer.save_model(str(OUTPUT_DIR))\n",
    "        print(\"Rank 0: Final model saved.\")\n",
    "    else:\n",
    "        print(f\"Rank {os.environ.get('RANK', 0)}: Not saving model.\")\n",
    "\n",
    "# End of training function definition\n",
    "print(\"‚úÖ Training function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-25T12:16:53.878673Z",
     "iopub.status.busy": "2025-10-25T12:16:53.877960Z",
     "iopub.status.idle": "2025-10-25T12:17:20.644180Z",
     "shell.execute_reply": "2025-10-25T12:17:20.642967Z",
     "shell.execute_reply.started": "2025-10-25T12:16:53.878651Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting notebook launcher for 2 processes...\n",
      "Launching training on 2 GPUs.\n",
      "Process 0: Loading base model Qwen/Qwen2.5-7B-Instruct...\n",
      "Process 1: Loading base model Qwen/Qwen2.5-7B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "628eebc8273342608f920f0cf5a13e82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "328264c9abf2403fb1b24012e1cb3fa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 0: Base model loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "W1025 12:17:20.223000 136841810080896 torch/multiprocessing/spawn.py:145] Terminating process 1207 via signal SIGTERM\n",
      "E1025 12:17:20.594000 136841810080896 torch/distributed/elastic/multiprocessing/api.py:695] failed (exitcode: 1) local_rank: 1 (pid: 1208) of fn: main_training_function (start_method: fork)\n",
      "E1025 12:17:20.594000 136841810080896 torch/distributed/elastic/multiprocessing/api.py:695] Traceback (most recent call last):\n",
      "E1025 12:17:20.594000 136841810080896 torch/distributed/elastic/multiprocessing/api.py:695]   File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 656, in _poll\n",
      "E1025 12:17:20.594000 136841810080896 torch/distributed/elastic/multiprocessing/api.py:695]     self._pc.join(-1)\n",
      "E1025 12:17:20.594000 136841810080896 torch/distributed/elastic/multiprocessing/api.py:695]   File \"/usr/local/lib/python3.11/dist-packages/torch/multiprocessing/spawn.py\", line 188, in join\n",
      "E1025 12:17:20.594000 136841810080896 torch/distributed/elastic/multiprocessing/api.py:695]     raise ProcessRaisedException(msg, error_index, failed_process.pid)\n",
      "E1025 12:17:20.594000 136841810080896 torch/distributed/elastic/multiprocessing/api.py:695] torch.multiprocessing.spawn.ProcessRaisedException: \n",
      "E1025 12:17:20.594000 136841810080896 torch/distributed/elastic/multiprocessing/api.py:695] \n",
      "E1025 12:17:20.594000 136841810080896 torch/distributed/elastic/multiprocessing/api.py:695] -- Process 1 terminated with the following error:\n",
      "E1025 12:17:20.594000 136841810080896 torch/distributed/elastic/multiprocessing/api.py:695] Traceback (most recent call last):\n",
      "E1025 12:17:20.594000 136841810080896 torch/distributed/elastic/multiprocessing/api.py:695]   File \"/usr/local/lib/python3.11/dist-packages/torch/multiprocessing/spawn.py\", line 75, in _wrap\n",
      "E1025 12:17:20.594000 136841810080896 torch/distributed/elastic/multiprocessing/api.py:695]     fn(i, *args)\n",
      "E1025 12:17:20.594000 136841810080896 torch/distributed/elastic/multiprocessing/api.py:695]   File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 580, in _wrap\n",
      "E1025 12:17:20.594000 136841810080896 torch/distributed/elastic/multiprocessing/api.py:695]     ret = record(fn)(*args_)\n",
      "E1025 12:17:20.594000 136841810080896 torch/distributed/elastic/multiprocessing/api.py:695]           ^^^^^^^^^^^^^^^^^^\n",
      "E1025 12:17:20.594000 136841810080896 torch/distributed/elastic/multiprocessing/api.py:695]   File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 347, in wrapper\n",
      "E1025 12:17:20.594000 136841810080896 torch/distributed/elastic/multiprocessing/api.py:695]     return f(*args, **kwargs)\n",
      "E1025 12:17:20.594000 136841810080896 torch/distributed/elastic/multiprocessing/api.py:695]            ^^^^^^^^^^^^^^^^^^\n",
      "E1025 12:17:20.594000 136841810080896 torch/distributed/elastic/multiprocessing/api.py:695]   File \"/tmp/ipykernel_109/2084375751.py\", line 32, in main_training_function\n",
      "E1025 12:17:20.594000 136841810080896 torch/distributed/elastic/multiprocessing/api.py:695]     model = AutoModelForCausalLM.from_pretrained(\n",
      "E1025 12:17:20.594000 136841810080896 torch/distributed/elastic/multiprocessing/api.py:695]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "E1025 12:17:20.594000 136841810080896 torch/distributed/elastic/multiprocessing/api.py:695]   File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrained\n",
      "E1025 12:17:20.594000 136841810080896 torch/distributed/elastic/multiprocessing/api.py:695]     return model_class.from_pretrained(\n",
      "E1025 12:17:20.594000 136841810080896 torch/distributed/elastic/multiprocessing/api.py:695]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "E1025 12:17:20.594000 136841810080896 torch/distributed/elastic/multiprocessing/api.py:695]   File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 3990, in from_pretrained\n",
      "E1025 12:17:20.594000 136841810080896 torch/distributed/elastic/multiprocessing/api.py:695]     dispatch_model(model, **device_map_kwargs)\n",
      "E1025 12:17:20.594000 136841810080896 torch/distributed/elastic/multiprocessing/api.py:695]   File \"/usr/local/lib/python3.11/dist-packages/accelerate/big_modeling.py\", line 419, in dispatch_model\n",
      "E1025 12:17:20.594000 136841810080896 torch/distributed/elastic/multiprocessing/api.py:695]     attach_align_device_hook_on_blocks(\n",
      "E1025 12:17:20.594000 136841810080896 torch/distributed/elastic/multiprocessing/api.py:695]   File \"/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py\", line 615, in attach_align_device_hook_on_blocks\n",
      "E1025 12:17:20.594000 136841810080896 torch/distributed/elastic/multiprocessing/api.py:695]     add_hook_to_module(module, hook)\n",
      "E1025 12:17:20.594000 136841810080896 torch/distributed/elastic/multiprocessing/api.py:695]   File \"/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py\", line 160, in add_hook_to_module\n",
      "E1025 12:17:20.594000 136841810080896 torch/distributed/elastic/multiprocessing/api.py:695]     module = hook.init_hook(module)\n",
      "E1025 12:17:20.594000 136841810080896 torch/distributed/elastic/multiprocessing/api.py:695]              ^^^^^^^^^^^^^^^^^^^^^^\n",
      "E1025 12:17:20.594000 136841810080896 torch/distributed/elastic/multiprocessing/api.py:695]   File \"/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py\", line 282, in init_hook\n",
      "E1025 12:17:20.594000 136841810080896 torch/distributed/elastic/multiprocessing/api.py:695]     set_module_tensor_to_device(module, name, self.execution_device, tied_params_map=self.tied_params_map)\n",
      "E1025 12:17:20.594000 136841810080896 torch/distributed/elastic/multiprocessing/api.py:695]   File \"/usr/local/lib/python3.11/dist-packages/accelerate/utils/modeling.py\", line 396, in set_module_tensor_to_device\n",
      "E1025 12:17:20.594000 136841810080896 torch/distributed/elastic/multiprocessing/api.py:695]     new_value = old_value.to(device)\n",
      "E1025 12:17:20.594000 136841810080896 torch/distributed/elastic/multiprocessing/api.py:695]                 ^^^^^^^^^^^^^^^^^^^^\n",
      "E1025 12:17:20.594000 136841810080896 torch/distributed/elastic/multiprocessing/api.py:695] torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU \n",
      "E1025 12:17:20.594000 136841810080896 torch/distributed/elastic/multiprocessing/api.py:695] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Training failed with error: \n",
      "============================================================\n",
      "main_training_function FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-10-25_12:17:19\n",
      "  host      : 5936555831bc\n",
      "  rank      : 1 (local_rank: 1)\n",
      "  exitcode  : 1 (pid: 1208)\n",
      "  error_file: /tmp/torchelastic_81n_230y/none_v3y56760/attempt_0/1/error.json\n",
      "  traceback : Traceback (most recent call last):\n",
      "    File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 347, in wrapper\n",
      "      return f(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "    File \"/tmp/ipykernel_109/2084375751.py\", line 32, in main_training_function\n",
      "      model = AutoModelForCausalLM.from_pretrained(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrained\n",
      "      return model_class.from_pretrained(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 3990, in from_pretrained\n",
      "      dispatch_model(model, **device_map_kwargs)\n",
      "    File \"/usr/local/lib/python3.11/dist-packages/accelerate/big_modeling.py\", line 419, in dispatch_model\n",
      "      attach_align_device_hook_on_blocks(\n",
      "    File \"/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py\", line 615, in attach_align_device_hook_on_blocks\n",
      "      add_hook_to_module(module, hook)\n",
      "    File \"/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py\", line 160, in add_hook_to_module\n",
      "      module = hook.init_hook(module)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^\n",
      "    File \"/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py\", line 282, in init_hook\n",
      "      set_module_tensor_to_device(module, name, self.execution_device, tied_params_map=self.tied_params_map)\n",
      "    File \"/usr/local/lib/python3.11/dist-packages/accelerate/utils/modeling.py\", line 396, in set_module_tensor_to_device\n",
      "      new_value = old_value.to(device)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU \n",
      "  \n",
      "============================================================\n"
     ]
    },
    {
     "ename": "ChildFailedError",
     "evalue": "\n============================================================\nmain_training_function FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2025-10-25_12:17:19\n  host      : 5936555831bc\n  rank      : 1 (local_rank: 1)\n  exitcode  : 1 (pid: 1208)\n  error_file: /tmp/torchelastic_81n_230y/none_v3y56760/attempt_0/1/error.json\n  traceback : Traceback (most recent call last):\n    File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 347, in wrapper\n      return f(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^\n    File \"/tmp/ipykernel_109/2084375751.py\", line 32, in main_training_function\n      model = AutoModelForCausalLM.from_pretrained(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrained\n      return model_class.from_pretrained(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 3990, in from_pretrained\n      dispatch_model(model, **device_map_kwargs)\n    File \"/usr/local/lib/python3.11/dist-packages/accelerate/big_modeling.py\", line 419, in dispatch_model\n      attach_align_device_hook_on_blocks(\n    File \"/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py\", line 615, in attach_align_device_hook_on_blocks\n      add_hook_to_module(module, hook)\n    File \"/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py\", line 160, in add_hook_to_module\n      module = hook.init_hook(module)\n               ^^^^^^^^^^^^^^^^^^^^^^\n    File \"/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py\", line 282, in init_hook\n      set_module_tensor_to_device(module, name, self.execution_device, tied_params_map=self.tied_params_map)\n    File \"/usr/local/lib/python3.11/dist-packages/accelerate/utils/modeling.py\", line 396, in set_module_tensor_to_device\n      new_value = old_value.to(device)\n                  ^^^^^^^^^^^^^^^^^^^^\n  torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU \n  \n============================================================",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mChildFailedError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_109/2858065881.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nüí° DEBUG HINT: The 'transformer_layer_cls_to_wrap' name might be wrong.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"   Check the model architecture. For Qwen2, it should be 'Qwen2DecoderLayer'.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"‚úÖ Training launcher finished.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_109/2858065881.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# This will launch the main_training_function on 2 GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mnotebook_launcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_training_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_processes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"‚ùå Training failed with error: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/launchers.py\u001b[0m in \u001b[0;36mnotebook_launcher\u001b[0;34m(function, args, num_processes, mixed_precision, use_port, master_addr, node_rank, num_nodes, rdzv_backend, rdzv_endpoint, rdzv_conf, rdzv_id, max_restarts, monitor_interval, log_line_prefix_template)\u001b[0m\n\u001b[1;32m    243\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mis_torch_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\">=\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mELASTIC_LOG_LINE_PREFIX_TEMPLATE_PYTORCH_VERSION\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                         \u001b[0mlaunch_config_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"log_line_prefix_template\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_line_prefix_template\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m                     \u001b[0melastic_launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLaunchConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mlaunch_config_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentrypoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProcessRaisedException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;34m\"Cannot re-initialize CUDA in forked subprocess\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlaunch_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_entrypoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py\u001b[0m in \u001b[0;36mlaunch_agent\u001b[0;34m(config, entrypoint, args)\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0;31m# @record will copy the first error (root cause)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0;31m# to the error file of the launcher process.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             raise ChildFailedError(\n\u001b[0m\u001b[1;32m    264\u001b[0m                 \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mentrypoint_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 \u001b[0mfailures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfailures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mChildFailedError\u001b[0m: \n============================================================\nmain_training_function FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2025-10-25_12:17:19\n  host      : 5936555831bc\n  rank      : 1 (local_rank: 1)\n  exitcode  : 1 (pid: 1208)\n  error_file: /tmp/torchelastic_81n_230y/none_v3y56760/attempt_0/1/error.json\n  traceback : Traceback (most recent call last):\n    File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 347, in wrapper\n      return f(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^\n    File \"/tmp/ipykernel_109/2084375751.py\", line 32, in main_training_function\n      model = AutoModelForCausalLM.from_pretrained(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrained\n      return model_class.from_pretrained(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 3990, in from_pretrained\n      dispatch_model(model, **device_map_kwargs)\n    File \"/usr/local/lib/python3.11/dist-packages/accelerate/big_modeling.py\", line 419, in dispatch_model\n      attach_align_device_hook_on_blocks(\n    File \"/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py\", line 615, in attach_align_device_hook_on_blocks\n      add_hook_to_module(module, hook)\n    File \"/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py\", line 160, in add_hook_to_module\n      module = hook.init_hook(module)\n               ^^^^^^^^^^^^^^^^^^^^^^\n    File \"/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py\", line 282, in init_hook\n      set_module_tensor_to_device(module, name, self.execution_device, tied_params_map=self.tied_params_map)\n    File \"/usr/local/lib/python3.11/dist-packages/accelerate/utils/modeling.py\", line 396, in set_module_tensor_to_device\n      new_value = old_value.to(device)\n                  ^^^^^^^^^^^^^^^^^^^^\n  torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU \n  \n============================================================"
     ]
    }
   ],
   "source": [
    "from accelerate import notebook_launcher\n",
    "import os, torch\n",
    "\n",
    "# Optional: quick sanity\n",
    "print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\" - GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "# Set environment variable for FSDP\n",
    "os.environ[\"ACCELERATE_USE_FSDP\"] = \"true\"\n",
    "\n",
    "print(\"Starting notebook launcher for 2 processes...\")\n",
    "\n",
    "# This will launch the main_training_function on 2 GPUs\n",
    "try:\n",
    "    notebook_launcher(main_training_function, num_processes=2)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Training failed with error: {e}\")\n",
    "    # This is to help debug FSDP issues\n",
    "    if \"transformer_layer_cls_to_wrap\" in str(e):\n",
    "        print(\"\\nüí° DEBUG HINT: The 'transformer_layer_cls_to_wrap' name might be wrong.\")\n",
    "        print(\"   Check the model architecture. For Qwen2, it should be 'Qwen2DecoderLayer'.\")\n",
    "    raise e\n",
    "\n",
    "print(\"‚úÖ Training launcher finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save the tokenizer\n",
    "print(f\"üíæ Saving tokenizer to {OUTPUT_DIR}...\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True, trust_remote_code=True)\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.save_pretrained(str(OUTPUT_DIR))\n",
    "    print(\"‚úÖ Tokenizer saved.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not save tokenizer: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Inference Test ---\n",
    "# We need to free memory first.\n",
    "# RESTART THE KERNEL after this cell if it fails on memory.\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üß™ RUNNING INFERENCE TEST...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "from peft import PeftModel\n",
    "from transformers import GenerationConfig\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    # Clear cache\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Load base model in 4-bit\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "    )\n",
    "    \n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\", # Load on one GPU for inference\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(str(OUTPUT_DIR), use_fast=True, trust_remote_code=True)\n",
    "\n",
    "    # Load the LoRA adapter\n",
    "    print(f\"Loading adapter from {OUTPUT_DIR}...\")\n",
    "    inf_model = PeftModel.from_pretrained(base_model, str(OUTPUT_DIR))\n",
    "    inf_model.eval()\n",
    "    print(\"‚úÖ Model and adapter loaded for inference.\")\n",
    "\n",
    "    # --- Test ---\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are Kayas, an intelligent AI assistant that helps users accomplish tasks by calling the appropriate tools.\\n\\nWhen given a command, respond with a JSON array of tool calls. Each tool call has:\\n- \\\"tool\\\": the tool name (e.g., \\\"filesystem.create_file\\\")\\n- \\\"args\\\": a dictionary of arguments\\n\\nAvailable tools:\\n- filesystem.create_file, process.start_program, uia.click_button, etc.\\n\\nRespond ONLY with valid JSON. No explanation, just the tool calls.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hey Kayas, can you open notepad and then find a file called 'todo.txt' on my desktop?\"}\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer([prompt], return_tensors='pt').to(inf_model.device)\n",
    "    \n",
    "    gen_config = GenerationConfig(\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.1, # Low temp for tool use\n",
    "        top_p=0.9,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- PROMPT ---\")\n",
    "    print(prompt)\n",
    "    \n",
    "    print(\"\\n--- GENERATION ---\")\n",
    "    with torch.inference_mode():\n",
    "        gen_outputs = inf_model.generate(**inputs, generation_config=gen_config)\n",
    "    \n",
    "    decoded = tokenizer.decode(gen_outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Print only the assistant's response\n",
    "    assistant_response = decoded.split(\"<|im_start|>assistant\")[1].replace(\"<|im_end|>\", \"\").strip()\n",
    "    print(assistant_response)\n",
    "    \n",
    "    # Try to parse the JSON\n",
    "    try:\n",
    "        json.loads(assistant_response)\n",
    "        print(\"\\n‚úÖ JSON is valid!\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ö†Ô∏è WARNING: Output is not valid JSON. {e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Inference test failed: {e}\")\n",
    "    print(\"   This might be an OOM error. Try restarting the session and running *only* this cell.\")\n",
    "\n",
    "print(\"\\nüéâ All done!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8568580,
     "sourceId": 13495685,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

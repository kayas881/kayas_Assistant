# âš¡ QUICK START GUIDE

## ğŸ¯ Goal
Train an AI "Brain" that converts voice commands into tool calls in 2-3 hours.

---

## ğŸ“‹ Prerequisites

### Hardware:
- **Training**: L4 24GB GPU (or similar cloud GPU)
- **Inference**: RTX 3050 4GB (your local machine)

### Software:
- Python 3.9+
- CUDA-enabled PyTorch
- 50GB free disk space

---

## ğŸš€ 5-Step Setup (Total: ~3 hours)

### Step 1: Setup (5 min)

```bash
# Clone/navigate to project
cd C:\Users\KAYAS\Desktop\kayasWorkPlace\kayas

# Navigate to brain training folder
cd brain_training

# Check prerequisites
python setup_check.py
```

**Expected output**: âœ… All checks passed!

---

### Step 2: Install Dependencies (5-10 min)

```bash
pip install -r requirements.txt
```

**Installs**:
- `torch` - Deep learning
- `transformers` - Hugging Face models
- `datasets` - Data loading
- `accelerate` - GPU optimization
- `bitsandbytes` - 4-bit quantization
- `peft` - LoRA adapters
- `trl` - Instruction tuning

---

### Step 3: Generate Training Data (1 min)

```bash
python generate_training_data.py
```

**Output**:
- `training_data/brain_training_base.jsonl` - 31 examples
- `training_data/brain_training_augmented.jsonl` - 119 examples

**What it does**:
- Creates synthetic examples: user commands â†’ JSON tool calls
- Augments with variations (rephrasing, politeness, casual)

---

### Step 4: Train the Brain (2-3 hours) â°

```bash
python finetune_brain.py
```

**What happens**:
1. Downloads Mistral-7B-Instruct (~14GB)
2. Loads in 4-bit mode (~4GB VRAM)
3. Fine-tunes with LoRA
4. Saves to `models/kayas_brain/final/`

**Monitor**:
```
Epoch 1/3
â”œâ”€ Step 0: Loss 2.5 (learning)
â”œâ”€ Step 100: Loss 1.2 (good)
â””â”€ Step 200: Loss 0.8 (better)
...
âœ… Training complete! Final eval loss: 0.12
```

**GPU Memory**:
- Should stay around 20-22GB
- If OOM: Edit `finetune_brain.py`, set `batch_size=1`

---

### Step 5: Test the Brain (5 min)

```bash
python test_brain.py
```

**Interactive mode**:
```
You: Create a file called todo.txt
AI Brain: [{"tool": "filesystem.create_file", "args": {"filename": "todo.txt", "content": ""}}]
âœ… Valid JSON with 1 tool call(s)

You: Open YouTube and search for Python tutorials
AI Brain: [{"tool": "browser.run_steps", "args": {...}}]
âœ… Valid JSON with 1 tool call(s)
```

**Type 'quit' to exit.**

---

## ğŸ“¦ Deploy to Local Machine (RTX 3050)

### Step 6: Package Model

On L4 machine:
```bash
cd models/kayas_brain/final
tar -czf kayas_brain.tar.gz *
```

**Size**: ~4GB (LoRA adapters only)

---

### Step 7: Transfer to Local

```bash
# Copy file to local machine (use USB, SCP, cloud storage, etc.)
# Example with SCP:
scp kayas_brain.tar.gz user@local-machine:~/

# Or upload to Google Drive/Dropbox and download
```

---

### Step 8: Extract on Local

On RTX 3050 machine:
```bash
mkdir ~/kayas_brain_model
cd ~/kayas_brain_model
tar -xzf ~/kayas_brain.tar.gz
```

---

### Step 9: Integrate with Kayas

Edit `src/agent/planner.py`:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import torch
import json

class BrainPlanner:
    def __init__(self, model_path: str = "~/kayas_brain_model"):
        print("ğŸ§  Loading AI Brain...")
        
        # Load tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            "mistralai/Mistral-7B-Instruct-v0.2"
        )
        
        # Load base model
        base_model = AutoModelForCausalLM.from_pretrained(
            "mistralai/Mistral-7B-Instruct-v0.2",
            device_map="auto",
            torch_dtype=torch.float16,
        )
        
        # Load LoRA adapters
        self.model = PeftModel.from_pretrained(base_model, model_path)
        self.model.eval()
        
        print("âœ… AI Brain ready!")
    
    def plan(self, user_command: str) -> List[Action]:
        """Convert user command to tool calls"""
        
        # System prompt
        system_prompt = """You are Kayas, an AI assistant that helps users by calling tools.
Respond with JSON array of tool calls: [{"tool": "...", "args": {...}}]"""
        
        # Format as chat
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_command}
        ]
        
        # Apply chat template
        prompt = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        # Tokenize
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
        
        # Generate
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=512,
                temperature=0.3,
                top_p=0.9,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id,
            )
        
        # Decode
        response = self.tokenizer.decode(
            outputs[0][inputs['input_ids'].shape[1]:],
            skip_special_tokens=True
        )
        
        # Parse JSON
        try:
            tools = json.loads(response)
            if isinstance(tools, dict):
                tools = [tools]
            return [Action(tool=t["tool"], args=t["args"]) for t in tools]
        except json.JSONDecodeError:
            print(f"âš ï¸ Invalid JSON: {response}")
            return []
```

---

### Step 10: Test End-to-End

```python
# In your main script
planner = BrainPlanner()

# Test
user_input = "Create a todo list"
actions = planner.plan(user_input)

print(f"User: {user_input}")
print(f"Brain: {actions}")
# Output: [Action(tool='filesystem.create_file', args={'filename': 'todo.txt', ...})]
```

---

## ğŸ‰ Done!

You now have:
- âœ… AI Brain trained on L4 GPU
- âœ… Deployed to local RTX 3050
- âœ… Integrated with Kayas assistant
- âœ… Fast inference (<100ms)
- âœ… Runs completely offline

---

## ğŸ”§ Troubleshooting

### Training Issues:

**CUDA Out of Memory**
```python
# Edit finetune_brain.py, line ~40
"batch_size": 1,  # was 2
"gradient_accumulation_steps": 16,  # was 8
```

**Model outputs invalid JSON**
```python
# Edit test_brain.py or planner.py, line ~XX
temperature=0.1,  # was 0.3 (lower = more deterministic)
```

**Training too slow**
- Already optimized for L4
- Expected: ~2-3 hours
- If slower: Check GPU utilization (`nvidia-smi`)

---

### Inference Issues:

**Out of memory on RTX 3050**
```python
# Load in 8-bit instead of float16
model = AutoModelForCausalLM.from_pretrained(
    "mistralai/Mistral-7B-Instruct-v0.2",
    device_map="auto",
    load_in_8bit=True,  # Even more memory efficient
)
```

**Slow inference**
- Expected: 50-100ms on RTX 3050
- If slower: Reduce `max_new_tokens` to 256

---

## ğŸ“Š Expected Results

### Training Metrics:
- Initial loss: ~2.5
- Final loss: ~0.1-0.2
- Training time: 2-3 hours
- GPU memory: 20-22GB

### Inference Performance:
- Latency: 50-100ms
- VRAM: 4GB
- Accuracy: >95% valid JSON

---

## ğŸ“ What You Built

```
USER SPEECH
    â†“
STT (Speech-to-Text)
    â†“
ğŸ§  AI BRAIN (your trained model)
    â†“ (JSON tool calls)
ROUTER
    â†“
EXECUTORS
    â†“
ğŸ‘ï¸ EYES (Accessibility API)
    â†“
ACTION EXECUTED âœ…
```

---

## ğŸ“š Next Steps

### Add More Tools:
1. Create executor in `src/executors/`
2. Register in `src/agent/actions.py`
3. Add examples to `generate_training_data.py`
4. Retrain with `python finetune_brain.py`

### Improve Accuracy:
1. Collect real user commands
2. Add them to training data
3. Retrain periodically

### Implement Eyes:
1. Install `pip install pywinauto`
2. Update `src/executors/uiautomation_exec.py`
3. Use Windows Accessibility API

---

## ğŸ†˜ Need Help?

- **Training**: Check `README.md` for detailed guide
- **Architecture**: Check `ARCHITECTURE.md` for diagrams
- **Summary**: Check `SUMMARY.md` for complete overview
- **Demo**: Run `python windows_eyes_demo.py`

---

**Time to completion: ~3 hours** â°
**Your AI assistant just got 10x smarter!** ğŸš€

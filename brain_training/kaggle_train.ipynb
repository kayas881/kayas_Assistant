{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06052de2",
   "metadata": {},
   "source": [
    "# Kayas Assistant â€” QLoRA Fine-tuning on Kaggle\n",
    "\n",
    "Use this notebook to fine-tune an instruct model on your enhanced 8k JSONL dataset with QLoRA.\n",
    "\n",
    "Prerequisites (in Kaggle Notebook settings):\n",
    "- Accelerator: GPU (T4/P100)\n",
    "- Internet: On (for pip and model download)\n",
    "- Add Data: Attach your dataset (e.g., a Kaggle Dataset or uploaded file) containing `mega_brain_dataset_8000_enhanced.jsonl`.\n",
    "\n",
    "Outputs will be saved under `/kaggle/working/brain-lora`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de3cf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment check\n",
    "import os, sys, platform, shutil\n",
    "print('Python:', sys.version)\n",
    "print('OS:', platform.platform())\n",
    "\n",
    "# Optional: list input data directory\n",
    "INPUT_DIR = '/kaggle/input'\n",
    "if os.path.exists(INPUT_DIR):\n",
    "    for root, dirs, files in os.walk(INPUT_DIR):\n",
    "        print('Found in /kaggle/input:', root)\n",
    "        # Only show top-level to keep output concise\n",
    "        break\n",
    "else:\n",
    "    print('Warning: /kaggle/input not found (are you running locally or forgot to attach data?)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec684e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "%pip install -q -U transformers datasets peft trl accelerate bitsandbytes einops safetensors huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8afcaf3",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "- Default base model is a small, Kaggle-friendly instruct model.\n",
    "- You can switch to a larger model if you have time/VRAM (e.g., Mistral-7B), but 7B can be slow or memory-constrained on Kaggle.\n",
    "- The notebook tries to auto-discover your data file under `/kaggle/input`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5884db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose Qwen 7B Instruct (multi-GPU friendly with QLoRA)\n",
    "# Alternatives for lower VRAM: 'Qwen/Qwen2.5-3B-Instruct', 'microsoft/Phi-3-mini-4k-instruct'\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "BASE_MODEL = 'Qwen/Qwen2.5-7B-Instruct'\n",
    "\n",
    "OUTPUT_DIR = Path('/kaggle/working/brain-lora')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# If you know the exact path, set it here; otherwise auto-discovery will run below.\n",
    "DATASET_FILE_OVERRIDE = ''  # e.g., '/kaggle/input/your-dataset/mega_brain_dataset_8000_enhanced.jsonl'\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "\n",
    "print('Base model:', BASE_MODEL)\n",
    "print('Output dir:', str(OUTPUT_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df15541e",
   "metadata": {},
   "source": [
    "## Locate dataset under /kaggle/input\n",
    "The code will auto-find `mega_brain_dataset_8000_enhanced.jsonl` if you attached it as a Kaggle Dataset or uploaded it via Add Data.\n",
    "You can also set `DATASET_FILE_OVERRIDE` to an explicit path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4baa584",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def find_dataset_file(filenames=None) -> str:\n",
    "    # Allow explicit override via cell variable or env var\n",
    "    if DATASET_FILE_OVERRIDE and Path(DATASET_FILE_OVERRIDE).exists():\n",
    "        return DATASET_FILE_OVERRIDE\n",
    "    env_override = os.environ.get('KAGGLE_DATASET_FILE', '')\n",
    "    if env_override and Path(env_override).exists():\n",
    "        return env_override\n",
    "\n",
    "    base = Path('/kaggle/input')\n",
    "    if not base.exists():\n",
    "        return ''\n",
    "\n",
    "    # Try common enhanced filenames first, then fall back\n",
    "    candidates_to_try = filenames or [\n",
    "        'mega_brain_dataset_10000_enhanced.jsonl',\n",
    "        'mega_brain_dataset_8000_enhanced.jsonl',\n",
    "        'mega_brain_dataset_10000.jsonl',\n",
    "        'mega_brain_dataset_8000.jsonl',\n",
    "    ]\n",
    "    for name in candidates_to_try:\n",
    "        found = list(base.rglob(name))\n",
    "        if found:\n",
    "            return str(found[0])\n",
    "\n",
    "    # Fallback: any mega_brain_dataset*.jsonl under input\n",
    "    any_found = list(base.rglob('mega_brain_dataset_*.jsonl'))\n",
    "    return str(any_found[0]) if any_found else ''\n",
    "\n",
    "DATA_PATH = find_dataset_file()\n",
    "print('Discovered dataset path:' , DATA_PATH if DATA_PATH else 'NOT FOUND')\n",
    "assert DATA_PATH, 'Dataset file not found. Attach data via \"Add Data\" and ensure a mega_brain_dataset_*.jsonl file exists, or set DATASET_FILE_OVERRIDE.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4621b564",
   "metadata": {},
   "source": [
    "## Load and prepare dataset\n",
    "The dataset is JSONL with one object per line containing a `messages` array (system/user/assistant). The notebook will auto-find `mega_brain_dataset_8000_enhanced.jsonl` under `/kaggle/input` if attached via Add Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5065caf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# T4 prefers fp16 compute for 4-bit quant\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# Re-load tokenizer here as each process may re-initialize\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# IMPORTANT: For DDP, do NOT set device_map here\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "model.config.use_cache = False  # safer with gradient checkpointing\n",
    "print('Model loaded (4-bit, fp16 compute).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f76afaf",
   "metadata": {},
   "source": [
    "## Load model and tokenizer (4-bit)\n",
    "We use 4-bit quantization (bitsandbytes) plus LoRA to train within Kaggle limits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22b16ca",
   "metadata": {},
   "source": [
    "## Configure LoRA and SFT Trainer\n",
    "We format chat samples using the tokenizer's chat template and train with TRL's SFTTrainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd55ecb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Common target modules for Qwen/LLaMA/Mistral families\n",
    "LORA_TARGET_MODULES = ['q_proj','k_proj','v_proj','o_proj','gate_proj','up_proj','down_proj']\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias='none',\n",
    "    task_type='CAUSAL_LM',\n",
    "    target_modules=LORA_TARGET_MODULES,\n",
    ")\n",
    "\n",
    "# Formatting function: render messages using chat template\n",
    "def formatting_func(example_batch):\n",
    "    texts = []\n",
    "    for msgs in example_batch['messages']:\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            msgs, tokenize=False, add_generation_prompt=False\n",
    "        )\n",
    "        texts.append(text)\n",
    "    return texts\n",
    "\n",
    "# We'll rebuild model/tokenizer and dataset inside the train loop for multi-GPU\n",
    "from datasets import Dataset, DatasetDict\n",
    "import json, random\n",
    "\n",
    "def train_loop():\n",
    "    import torch\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "    from trl import SFTTrainer\n",
    "    from transformers import TrainingArguments\n",
    "\n",
    "    # Re-seed per process\n",
    "    random.seed(SEED)\n",
    "\n",
    "    # Load tokenizer/model (per process)\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "    )\n",
    "    tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "    if tok.pad_token_id is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    mdl = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        quantization_config=bnb_config,\n",
    "    )\n",
    "    mdl.config.use_cache = False\n",
    "\n",
    "    # Load dataset again inside process\n",
    "    def load_jsonl_messages(path: str):\n",
    "        rows = []\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                obj = json.loads(line)\n",
    "                if 'messages' in obj:\n",
    "                    rows.append({'messages': obj['messages']})\n",
    "        return rows\n",
    "\n",
    "    raw_rows = load_jsonl_messages(DATA_PATH)\n",
    "    random.shuffle(raw_rows)\n",
    "    val_size = max(1, int(0.025 * len(raw_rows)))\n",
    "    val_rows = raw_rows[:val_size]\n",
    "    train_rows = raw_rows[val_size:]\n",
    "\n",
    "    ds_train = Dataset.from_list(train_rows)\n",
    "    ds_val = Dataset.from_list(val_rows)\n",
    "\n",
    "    # Training args optimized for 2x T4 with QLoRA 7B\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=str(OUTPUT_DIR),\n",
    "        per_device_train_batch_size=2,\n",
    "        per_device_eval_batch_size=2,\n",
    "        gradient_accumulation_steps=8,\n",
    "        learning_rate=2e-4,\n",
    "        logging_steps=10,\n",
    "        num_train_epochs=2,\n",
    "        warmup_ratio=0.03,\n",
    "        fp16=True,\n",
    "        bf16=False,\n",
    "        lr_scheduler_type='cosine',\n",
    "        gradient_checkpointing=True,\n",
    "        save_strategy='no',\n",
    "        evaluation_strategy='steps',\n",
    "        eval_steps=250,\n",
    "        optim='paged_adamw_8bit',\n",
    "        group_by_length=True,\n",
    "        ddp_find_unused_parameters=False,\n",
    "        report_to=[],\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=mdl,\n",
    "        tokenizer=tok,\n",
    "        train_dataset=ds_train,\n",
    "        eval_dataset=ds_val,\n",
    "        peft_config=lora_config,\n",
    "        formatting_func=lambda batch: [\n",
    "            tok.apply_chat_template(m, tokenize=False, add_generation_prompt=False) for m in batch['messages']\n",
    "        ],\n",
    "        max_seq_length=1024,\n",
    "        packing=True,\n",
    "        args=training_args,\n",
    "    )\n",
    "\n",
    "    train_result = trainer.train()\n",
    "\n",
    "    # Save only on main process\n",
    "    if trainer.accelerator.is_main_process:\n",
    "        trainer.model.save_pretrained(str(OUTPUT_DIR))\n",
    "        tok.save_pretrained(str(OUTPUT_DIR))\n",
    "    return train_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5a4b09",
   "metadata": {},
   "source": [
    "## Train\n",
    "This may take a few hours depending on the GPU and dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712d7f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kick off training (uses Accelerate under the hood; will leverage multiple GPUs if available)\n",
    "result = train_loop()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fe08fc",
   "metadata": {},
   "source": [
    "## Save adapter and tokenizer\n",
    "The LoRA adapter and tokenizer are saved under `/kaggle/working/brain-lora`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443e752e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models and tokenizer are already saved inside train_loop on main process\n",
    "from pathlib import Path\n",
    "print('Artifacts present:', list(Path(OUTPUT_DIR).glob('*')))\n",
    "print('Saved to', str(OUTPUT_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0ef663",
   "metadata": {},
   "source": [
    "## Quick inference test\n",
    "Load the base model with the trained adapter and generate a short response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bcd137",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import TextIteratorStreamer\n",
    "import threading, time\n",
    "\n",
    "# Reload for inference (4-bit + adapter)\n",
    "base = AutoModelForCausalLM.from_pretrained(BASE_MODEL, quantization_config=bnb_config, device_map='auto')\n",
    "tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "if tok.pad_token_id is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "inf_model = PeftModel.from_pretrained(base, str(OUTPUT_DIR))\n",
    "inf_model.eval()\n",
    "\n",
    "# Sample conversation\n",
    "messages = [\n",
    "    { 'role': 'system', 'content': 'You are a helpful assistant that can plan actions and clarify ambiguity.' },\n",
    "    { 'role': 'user', 'content': 'open vscode then search repo readme for setup' }\n",
    "]\n",
    "prompt = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tok([prompt], return_tensors='pt').to(inf_model.device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    gen = inf_model.generate(**inputs, max_new_tokens=128, do_sample=True, temperature=0.7, top_p=0.9)\n",
    "print(tok.decode(gen[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d4854f",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- For faster runs, reduce `max_seq_length` to 768 and/or `num_train_epochs` to 1.\n",
    "- If you have more time/VRAM, try `num_train_epochs=2` or switch `BASE_MODEL` to a larger instruct model.\n",
    "- To export, download `/kaggle/working/brain-lora` as an artifact or push to the Hugging Face Hub if you add a token."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

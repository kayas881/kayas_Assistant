{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84b3fd3d-8873-4d22-8f57-531039bf3847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Dependencies installed!\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "!pip install -q --upgrade pip\n",
    "!pip install -q transformers accelerate peft bitsandbytes datasets pillow tqdm torch torchvision torchaudio huggingface_hub\n",
    "\n",
    "# Clear GPU cache after installs\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abd80885-919e-434b-a02f-d15dd773a781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GPU detected: NVIDIA L4\n",
      "   VRAM: 23.67 GB\n",
      "\n",
      "üêç Python: 3.12.7 | packaged by conda-forge | (main, Oct  4 2024, 16:05:46) [GCC 13.3.0]\n",
      "üî• PyTorch: 2.9.0+cu128\n",
      "üíæ CUDA: 12.8\n",
      "‚ö° cuDNN: 91002\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No GPU detected - training will be very slow!\")\n",
    "\n",
    "# Check PyTorch and CUDA versions\n",
    "print(f\"\\nüêç Python: {os.sys.version}\")\n",
    "print(f\"üî• PyTorch: {torch.__version__}\")\n",
    "try:\n",
    "    print(f\"üíæ CUDA: {torch.version.cuda}\")\n",
    "    print(f\"‚ö° cuDNN: {torch.backends.cudnn.version()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not retrieve CUDA/cuDNN version: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c11dc3-ebcc-4afb-9ffb-9519060e42b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è  Configuration (L4 24GB Optimized for 2B Model):\n",
      "   Dataset: Local folder 'training_data_auto'\n",
      "   Model: Qwen/Qwen2-VL-2B-Instruct\n",
      "   Output (Adapter): ui_model_2B_finetuned_qlora\n",
      "   Batch size: 4 (Effective: 16)\n",
      "   Max Length: 8192 tokens\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "# Dataset (Using local folder)\n",
    "DATA_FOLDER = Path(\"./training_data_auto\") # <-- Your local folder name\n",
    "\n",
    "# Model\n",
    "MODEL_NAME = \"Qwen/Qwen2-VL-2B-Instruct\" # <-- 2B model\n",
    "\n",
    "# Training Output\n",
    "OUTPUT_DIR = Path(\"./ui_model_2B_finetuned_qlora\")  \n",
    "MERGED_OUTPUT_DIR = Path(\"./ui_model_2B_merged\") \n",
    "\n",
    "# L4 24GB Optimized Settings - FASTER training with smart text limiting\n",
    "BATCH_SIZE = 2                  # Can use 2 now with shorter text\n",
    "GRADIENT_ACCUMULATION_STEPS = 8 # Back to 8 for effective batch of 16\n",
    "NUM_EPOCHS = 2                  \n",
    "LEARNING_RATE = 3e-4            \n",
    "# Limit response text length but keep full image\n",
    "MAX_ELEMENT_TEXT_CHARS = 500    # Limit text description to ~500 chars (keeps images intact)\n",
    "MAX_SEQ_LENGTH = 8192           # Reasonable max with limited text\n",
    "\n",
    "# Other Settings\n",
    "MAX_SAMPLES = None              # Use None for the full dataset\n",
    "SAVE_STEPS = 500                \n",
    "LOGGING_STEPS = 50              \n",
    "\n",
    "# --- End Configuration ---\n",
    "\n",
    "# Create directories\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "MERGED_OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "print(\"‚öôÔ∏è  Configuration (L4 24GB Optimized for 2B Model - SPEED MODE):\")\n",
    "print(f\"   Dataset: Local folder '{DATA_FOLDER}'\")\n",
    "print(f\"   Model: {MODEL_NAME}\")\n",
    "print(f\"   Output (Adapter): {OUTPUT_DIR}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE} (Effective: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS})\")\n",
    "print(f\"   Max text chars: {MAX_ELEMENT_TEXT_CHARS} (keeps image, limits description)\")\n",
    "print(f\"   Max seq length: {MAX_SEQ_LENGTH} tokens\")\n",
    "print(f\"   ‚ö° SPEED MODE: ~2-3x faster with smarter text limiting!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db54712e-4a81-47bd-883e-115b7bf9de39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found 'dataset.json' at: training_data_auto/dataset.json\n",
      "‚úÖ Image base directory set to: training_data_auto\n",
      "üîÑ Loading data from 'training_data_auto/dataset.json'...\n",
      "   Found 5,114 samples in JSON.\n",
      "\n",
      "‚úÖ Train samples: 4,602\n",
      "‚úÖ Validation samples: 512\n",
      "\n",
      "üîÑ Loading processor...\n",
      "‚úÖ Processor loaded.\n",
      "\n",
      "üîÑ Applying preprocessing function to datasets...\n",
      "   Using batched=False (This will take a while but is RAM-safe)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40bd6208f3244124ba53a9324314d2f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4602 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Assistant marker not found in sample. Masking first half.\n",
      "Warning: Assistant marker not found in sample. Masking first half.\n",
      "Warning: Assistant marker not found in sample. Masking first half.\n",
      "Warning: Assistant marker not found in sample. Masking first half.\n",
      "Warning: Assistant marker not found in sample. Masking first half.\n",
      "Warning: Assistant marker not found in sample. Masking first half.\n",
      "Warning: Assistant marker not found in sample. Masking first half.\n",
      "Warning: Assistant marker not found in sample. Masking first half.\n",
      "Warning: Assistant marker not found in sample. Masking first half.\n",
      "Warning: Assistant marker not found in sample. Masking first half.\n",
      "Warning: Assistant marker not found in sample. Masking first half.\n",
      "Warning: Assistant marker not found in sample. Masking first half.\n",
      "Warning: Assistant marker not found in sample. Masking first half.\n",
      "Warning: Assistant marker not found in sample. Masking first half.\n",
      "Warning: Assistant marker not found in sample. Masking first half.\n",
      "Warning: Assistant marker not found in sample. Masking first half.\n",
      "Warning: Assistant marker not found in sample. Masking first half.\n",
      "Warning: Assistant marker not found in sample. Masking first half.\n",
      "Warning: Assistant marker not found in sample. Masking first half.\n",
      "Warning: Assistant marker not found in sample. Masking first half.\n",
      "Warning: Assistant marker not found in sample. Masking first half.\n",
      "Warning: Assistant marker not found in sample. Masking first half.\n",
      "Warning: Assistant marker not found in sample. Masking first half.\n",
      "Warning: Assistant marker not found in sample. Masking first half.\n",
      "Warning: Assistant marker not found in sample. Masking first half.\n",
      "Warning: Assistant marker not found in sample. Masking first half.\n",
      "Warning: Assistant marker not found in sample. Masking first half.\n",
      "Warning: Assistant marker not found in sample. Masking first half.\n",
      "Warning: Assistant marker not found in sample. Masking first half.\n",
      "Warning: Assistant marker not found in sample. Masking first half.\n",
      "Warning: Assistant marker not found in sample. Masking first half.\n",
      "Warning: Assistant marker not found in sample. Masking first half.\n",
      "Warning: Assistant marker not found in sample. Masking first half.\n",
      "Warning: Assistant marker not found in sample. Masking first half.\n",
      "Warning: Assistant marker not found in sample. Masking first half.\n",
      "Warning: Assistant marker not found in sample. Masking first half.\n",
      "Warning: Assistant marker not found in sample. Masking first half.\n",
      "Warning: Assistant marker not found in sample. Masking first half.\n",
      "Warning: Assistant marker not found in sample. Masking first half.\n",
      "Warning: Assistant marker not found in sample. Masking first half.\n",
      "Warning: Assistant marker not found in sample. Masking first half.\n",
      "Warning: Assistant marker not found in sample. Masking first half.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 136\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# üí°üí°üí° THE FIX IS HERE üí°üí°üí°\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# We set batched=False. This is MUCH slower but will NOT crash your RAM.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Using batched=False (This will take a while but is RAM-safe)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 136\u001b[0m train_dataset_processed \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m val_dataset_processed \u001b[38;5;241m=\u001b[39m val_dataset\u001b[38;5;241m.\u001b[39mmap(preprocess_function, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# Filter out any samples that failed (where we returned None)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/datasets/arrow_dataset.py:562\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    555\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    560\u001b[0m }\n\u001b[1;32m    561\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 562\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    563\u001b[0m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/datasets/arrow_dataset.py:3332\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[0m\n\u001b[1;32m   3330\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3331\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m unprocessed_kwargs \u001b[38;5;129;01min\u001b[39;00m unprocessed_kwargs_per_job:\n\u001b[0;32m-> 3332\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43munprocessed_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   3333\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcheck_if_shard_done\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3335\u001b[0m \u001b[38;5;66;03m# Avoids PermissionError on Windows (the error: https://github.com/huggingface/datasets/actions/runs/4026734820/jobs/6921621805)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/datasets/arrow_dataset.py:3664\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001b[0m\n\u001b[1;32m   3662\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batched:\n\u001b[1;32m   3663\u001b[0m     _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m-> 3664\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miter_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_iterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   3665\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mupdate_data\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   3666\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/datasets/arrow_dataset.py:3638\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.iter_outputs\u001b[0;34m(shard_iterable)\u001b[0m\n\u001b[1;32m   3636\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3637\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[0;32m-> 3638\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m i, \u001b[43mapply_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/datasets/arrow_dataset.py:3561\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function\u001b[0;34m(pa_inputs, indices, offset)\u001b[0m\n\u001b[1;32m   3559\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[39;00m\n\u001b[1;32m   3560\u001b[0m inputs, fn_args, additional_args, fn_kwargs \u001b[38;5;241m=\u001b[39m prepare_inputs(pa_inputs, indices, offset\u001b[38;5;241m=\u001b[39moffset)\n\u001b[0;32m-> 3561\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3562\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prepare_outputs(pa_inputs, inputs, processed_inputs)\n",
      "Cell \u001b[0;32mIn[6], line 94\u001b[0m, in \u001b[0;36mpreprocess_function\u001b[0;34m(example)\u001b[0m\n\u001b[1;32m     90\u001b[0m text \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mapply_chat_template(messages, tokenize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# --- Process Single Sample ---\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Note: We pass [image] and [text] as lists, but they only contain one item\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlongest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Padding is trivial for a single item\u001b[39;49;00m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_SEQ_LENGTH\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Squeeze batch dimensions that processor adds\u001b[39;00m\n\u001b[1;32m    104\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/models/qwen2_vl/processing_qwen2_vl.py:144\u001b[0m, in \u001b[0;36mQwen2VLProcessor.__call__\u001b[0;34m(self, images, text, videos, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m image_inputs \u001b[38;5;241m=\u001b[39m videos_inputs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m     image_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moutput_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimages_kwargs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     image_grid_thw \u001b[38;5;241m=\u001b[39m image_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_grid_thw\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m videos \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/image_processing_utils_fast.py:732\u001b[0m, in \u001b[0;36mBaseImageProcessorFast.__call__\u001b[0;34m(self, images, *args, **kwargs)\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images: ImageInput, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Unpack[DefaultFastImageProcessorKwargs]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchFeature:\n\u001b[0;32m--> 732\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/models/qwen2_vl/image_processing_qwen2_vl_fast.py:141\u001b[0m, in \u001b[0;36mQwen2VLImageProcessorFast.preprocess\u001b[0;34m(self, images, videos, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess\u001b[39m(\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Unpack[Qwen2VLFastImageProcessorKwargs],\n\u001b[1;32m    140\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchFeature:\n\u001b[0;32m--> 141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/image_processing_utils_fast.py:757\u001b[0m, in \u001b[0;36mBaseImageProcessorFast.preprocess\u001b[0;34m(self, images, *args, **kwargs)\u001b[0m\n\u001b[1;32m    754\u001b[0m \u001b[38;5;66;03m# Pop kwargs that are not needed in _preprocess\u001b[39;00m\n\u001b[1;32m    755\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_format\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 757\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_preprocess_image_like_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_convert_rgb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_convert_rgb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/models/qwen2_vl/image_processing_qwen2_vl_fast.py:163\u001b[0m, in \u001b[0;36mQwen2VLImageProcessorFast._preprocess_image_like_inputs\u001b[0;34m(self, images, videos, do_convert_rgb, input_data_format, device, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    160\u001b[0m     images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_image_like_inputs(\n\u001b[1;32m    161\u001b[0m         images\u001b[38;5;241m=\u001b[39mimages, do_convert_rgb\u001b[38;5;241m=\u001b[39mdo_convert_rgb, input_data_format\u001b[38;5;241m=\u001b[39minput_data_format, device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[1;32m    162\u001b[0m     )\n\u001b[0;32m--> 163\u001b[0m     batch_feature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_preprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m videos \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Qwen2VLImageProcessorFast` works only with image inputs and doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt process videos anymore. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is a deprecated behavior and will be removed in v5.0. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour videos should be forwarded to `Qwen2VLVideoProcessor`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    169\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/models/qwen2_vl/image_processing_qwen2_vl_fast.py:257\u001b[0m, in \u001b[0;36mQwen2VLImageProcessorFast._preprocess\u001b[0;34m(self, images, do_resize, size, interpolation, do_rescale, rescale_factor, do_normalize, image_mean, image_std, patch_size, temporal_patch_size, merge_size, disable_grouping, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;66;03m# Reorder dimensions to group grid and patch information for subsequent flattening.\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;66;03m# (batch, grid_t, grid_h, grid_w, merge_h, merge_w, channel, temp_patch_size, patch_h, patch_w)\u001b[39;00m\n\u001b[1;32m    256\u001b[0m patches \u001b[38;5;241m=\u001b[39m patches\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m9\u001b[39m)\n\u001b[0;32m--> 257\u001b[0m flatten_patches \u001b[38;5;241m=\u001b[39m \u001b[43mpatches\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrid_t\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgrid_h\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgrid_w\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchannel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtemporal_patch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m processed_images_grouped[shape] \u001b[38;5;241m=\u001b[39m flatten_patches\n\u001b[1;32m    264\u001b[0m processed_grids[shape] \u001b[38;5;241m=\u001b[39m [[grid_t, grid_h, grid_w]] \u001b[38;5;241m*\u001b[39m batch_size\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from transformers import AutoProcessor\n",
    "import torch\n",
    "import json\n",
    "from PIL import Image\n",
    "from datasets import Dataset\n",
    "import os\n",
    "\n",
    "# --- 1. DEFINE PATHS ---\n",
    "json_path = DATA_FOLDER / \"dataset.json\"\n",
    "IMAGE_BASE_PATH = DATA_FOLDER\n",
    "\n",
    "if not json_path.exists():\n",
    "    print(f\"‚ùå ERROR: Cannot find 'dataset.json' at {json_path}\")\n",
    "    raise FileNotFoundError(f\"Missing {json_path}\")\n",
    "else:\n",
    "    print(f\"‚úÖ Found 'dataset.json' at: {json_path}\")\n",
    "    print(f\"‚úÖ Image base directory set to: {IMAGE_BASE_PATH}\")\n",
    "\n",
    "# --- 2. Load Local JSON ---\n",
    "print(f\"üîÑ Loading data from '{json_path}'...\")\n",
    "with open(json_path, 'r') as f:\n",
    "    local_data = json.load(f)\n",
    "\n",
    "if 'samples' not in local_data:\n",
    "     print(f\"‚ùå ERROR: 'samples' key not found in {json_path}.\")\n",
    "     raise KeyError(\"'samples' key not found in dataset.json\")\n",
    "\n",
    "data_list = local_data['samples']\n",
    "print(f\"   Found {len(data_list):,} samples in JSON.\")\n",
    "\n",
    "# --- 3. Convert to Hugging Face Dataset object (metadata only) ---\n",
    "# Keep only lightweight fields and transform on-the-fly to avoid RAM blowup\n",
    "raw_dataset = Dataset.from_list([\n",
    "    {\"screenshot\": s.get(\"screenshot\"), \"elements\": s.get(\"elements\", [])}\n",
    "    for s in data_list\n",
    "])\n",
    "\n",
    "if MAX_SAMPLES is not None:\n",
    "    raw_dataset = raw_dataset.select(range(min(MAX_SAMPLES, len(raw_dataset))))\n",
    "    print(f\"\\n‚úÇÔ∏è Using a subset of {len(raw_dataset):,} samples for training.\")\n",
    "\n",
    "split_dataset = raw_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = split_dataset['train']\n",
    "val_dataset = split_dataset['test']\n",
    "\n",
    "print(f\"\\n‚úÖ Train samples: {len(train_dataset):,}\")\n",
    "print(f\"‚úÖ Validation samples: {len(val_dataset):,}\")\n",
    "\n",
    "# --- 4. Load Processor ---\n",
    "print(\"\\nüîÑ Loading processor...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "print(\"‚úÖ Processor loaded.\")\n",
    "\n",
    "# --- 5. On-the-fly transform to prevent materializing tensors ---\n",
    "_WARN_STATE = {\"miss\": 0}\n",
    "\n",
    "def _find_subsequence(seq, subseq):\n",
    "    if not subseq:\n",
    "        return -1\n",
    "    n, m = len(seq), len(subseq)\n",
    "    if m > n:\n",
    "        return -1\n",
    "    for i in range(0, n - m + 1):\n",
    "        if seq[i:i + m] == subseq:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "\n",
    "def _normalize_elements(element_data):\n",
    "    \"\"\"Flatten and keep only dict-like UI element entries.\"\"\"\n",
    "    out = []\n",
    "    if isinstance(element_data, dict):\n",
    "        out.append(element_data)\n",
    "    elif isinstance(element_data, list):\n",
    "        for item in element_data:\n",
    "            if isinstance(item, dict):\n",
    "                out.append(item)\n",
    "            elif isinstance(item, list):\n",
    "                for sub in item:\n",
    "                    if isinstance(sub, dict):\n",
    "                        out.append(sub)\n",
    "            # ignore other types\n",
    "    return out\n",
    "\n",
    "\n",
    "def _process_one_sample(image_filename, raw_elements):\n",
    "    \"\"\"Process a single sample (not batched).\"\"\"\n",
    "    image_path = IMAGE_BASE_PATH / image_filename\n",
    "\n",
    "    try:\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "    except Exception as e:\n",
    "        if _WARN_STATE[\"miss\"] < 5:\n",
    "            print(f\"Warning: Skipping sample. Could not load image {image_path}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    elements = _normalize_elements(raw_elements)\n",
    "\n",
    "    if elements:\n",
    "        instruction = \"Describe the UI elements visible in this Windows interface.\"\n",
    "        # Build element description with character limit for speed\n",
    "        element_count = len(elements)\n",
    "        preview = []\n",
    "        char_count = 0\n",
    "        for e in elements[:20]:  # Check up to 20 elements\n",
    "            etype = e.get('type', 'Control') if isinstance(e, dict) else 'Control'\n",
    "            ename = e.get('name', '') if isinstance(e, dict) else ''\n",
    "            elem_str = f\"{etype} '{ename}'\"\n",
    "            if char_count + len(elem_str) + 2 > MAX_ELEMENT_TEXT_CHARS:\n",
    "                break  # Stop if we'd exceed limit\n",
    "            preview.append(elem_str)\n",
    "            char_count += len(elem_str) + 2  # +2 for \"; \"\n",
    "        \n",
    "        element_list_str = \"; \".join(preview)\n",
    "        if len(preview) < element_count:\n",
    "            response = f\"This interface contains {element_count} UI elements including: {element_list_str}...\"\n",
    "        else:\n",
    "            response = f\"This interface contains {element_count} UI elements. Key elements: {element_list_str}.\"\n",
    "    else:\n",
    "        instruction = \"What do you see in this Windows interface?\"\n",
    "        response = \"This is a Windows interface screenshot showing UI controls.\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": instruction}]},\n",
    "        {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": response}]}\n",
    "    ]\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "\n",
    "    # Process with smart truncation - images preserved, text may be cut\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=[image],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=MAX_SEQ_LENGTH\n",
    "    )\n",
    "    inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "\n",
    "    labels = inputs[\"input_ids\"].clone()\n",
    "    resp_ids = processor.tokenizer(response, add_special_tokens=False)[\"input_ids\"]\n",
    "    seq_ids = labels.tolist()\n",
    "    start_idx = _find_subsequence(seq_ids, resp_ids) if resp_ids else -1\n",
    "    if start_idx != -1:\n",
    "        labels[:start_idx] = -100\n",
    "    else:\n",
    "        cut = int(labels.shape[0] * 0.7)\n",
    "        labels[:cut] = -100\n",
    "        _WARN_STATE[\"miss\"] += 1\n",
    "        if _WARN_STATE[\"miss\"] <= 5 or _WARN_STATE[\"miss\"] % 500 == 0:\n",
    "            print(\"Warning: Could not locate assistant response tokens reliably. Applying fallback mask.\")\n",
    "\n",
    "    inputs[\"labels\"] = labels\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def _transform(examples):\n",
    "    \"\"\"Transform function that handles batched data from set_transform.\"\"\"\n",
    "    # Check if batched (dict with lists) or single example (dict with single values)\n",
    "    is_batched = isinstance(examples.get('screenshot'), list)\n",
    "    \n",
    "    if is_batched:\n",
    "        # Process each sample in the batch\n",
    "        results = []\n",
    "        for i in range(len(examples['screenshot'])):\n",
    "            result = _process_one_sample(\n",
    "                examples['screenshot'][i],\n",
    "                examples['elements'][i]\n",
    "            )\n",
    "            if result is not None:\n",
    "                results.append(result)\n",
    "            else:\n",
    "                # Return empty tensor for failed samples\n",
    "                results.append({\"input_ids\": torch.tensor([], dtype=torch.long)})\n",
    "        \n",
    "        # Return batched results\n",
    "        if not results:\n",
    "            return {\"input_ids\": [torch.tensor([], dtype=torch.long)]}\n",
    "        \n",
    "        # Collate the results into batch format\n",
    "        batch = {key: [r[key] for r in results] for key in results[0].keys()}\n",
    "        return batch\n",
    "    else:\n",
    "        # Single example\n",
    "        result = _process_one_sample(examples['screenshot'], examples['elements'])\n",
    "        if result is None:\n",
    "            return {\"input_ids\": torch.tensor([], dtype=torch.long)}\n",
    "        return result\n",
    "\n",
    "train_dataset.set_transform(_transform)\n",
    "val_dataset.set_transform(_transform)\n",
    "\n",
    "print(\"‚úÖ Using on-the-fly transforms to minimize RAM usage (no precomputed tensors).\")\n",
    "print(f\"‚ö° SPEED MODE: Text limited to ~{MAX_ELEMENT_TEXT_CHARS} chars, sequences to {MAX_SEQ_LENGTH} tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ad1d91-fbf5-4898-819c-9a661bdc9403",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "print(f\"üì• Loading base model {MODEL_NAME} with 4-bit quantization...\\n\")\n",
    "\n",
    "# 4-bit quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load base model\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\", # Automatically place layers on GPU\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Processor is already loaded\n",
    "\n",
    "# Prepare model for K-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=16, \n",
    "    lora_alpha=32, \n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\" \n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print parameter summary\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\n‚úÖ Model prepared for QLoRA training!\")\n",
    "print(f\"üìä Trainable LoRA parameters: {trainable_params:,} ({trainable_params/total_params*100:.2f}% of total)\")\n",
    "\n",
    "# Check memory usage\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üíæ Initial GPU Memory Used: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337dce77-ecd2-40c6-abc9-6c4b19c28f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "print(\"‚öôÔ∏è Setting up Training Arguments...\")\n",
    "\n",
    "# Training Arguments (L4 24GB / 2B Model - SPEED optimized)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(OUTPUT_DIR),\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE, # 2\n",
    "    per_device_eval_batch_size=BATCH_SIZE,  # 2\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS, # 8\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    lr_scheduler_type=\"cosine\", \n",
    "    warmup_ratio=0.03, \n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    save_strategy=\"steps\", \n",
    "    save_steps=SAVE_STEPS,\n",
    "    eval_strategy=\"steps\", \n",
    "    eval_steps=SAVE_STEPS, \n",
    "    save_total_limit=2, \n",
    "    load_best_model_at_end=True, \n",
    "    metric_for_best_model=\"eval_loss\", \n",
    "    greater_is_better=False, \n",
    "    fp16=torch.cuda.is_available(), \n",
    "    optim=\"paged_adamw_8bit\", \n",
    "    gradient_checkpointing=True,  # Keep enabled for safety\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    report_to=\"none\", \n",
    "    remove_unused_columns=False, \n",
    "    dataloader_num_workers=0, \n",
    "    dataloader_pin_memory=True,\n",
    ")\n",
    "\n",
    "# Collator: handles lists of tensors from batched transform\n",
    "from typing import List\n",
    "\n",
    "def custom_collator(features: List[dict]):\n",
    "    \"\"\"Collate features that may contain lists of tensors or single tensors.\"\"\"\n",
    "    # Flatten if features contain lists (from batched transform)\n",
    "    flattened = []\n",
    "    for feat in features:\n",
    "        if isinstance(feat.get(\"input_ids\"), list):\n",
    "            # Batched transform output: expand the nested batch\n",
    "            for i in range(len(feat[\"input_ids\"])):\n",
    "                sample = {k: v[i] if isinstance(v, list) else v for k, v in feat.items()}\n",
    "                if sample.get(\"input_ids\") is not None and sample[\"input_ids\"].numel() > 0:\n",
    "                    flattened.append(sample)\n",
    "        else:\n",
    "            # Single sample\n",
    "            if feat.get(\"input_ids\") is not None and feat[\"input_ids\"].numel() > 0:\n",
    "                flattened.append(feat)\n",
    "    \n",
    "    if len(flattened) == 0:\n",
    "        return {\n",
    "            \"input_ids\": torch.zeros((0,1), dtype=torch.long),\n",
    "            \"attention_mask\": torch.zeros((0,1), dtype=torch.long),\n",
    "            \"pixel_values\": torch.zeros((0,3,1,1), dtype=torch.float32),\n",
    "            \"labels\": torch.zeros((0,1), dtype=torch.long),\n",
    "        }\n",
    "\n",
    "    input_ids = [f[\"input_ids\"] for f in flattened]\n",
    "    labels = [f[\"labels\"] for f in flattened]\n",
    "    attention_mask = [f.get(\"attention_mask\") for f in flattened]\n",
    "    pixel_values = [f[\"pixel_values\"] for f in flattened]\n",
    "    \n",
    "    # Handle image_grid_thw if present (Qwen2-VL specific)\n",
    "    # Each sample has shape (1, 3) -> stack to (batch_size, 3)\n",
    "    image_grid_thw = [f[\"image_grid_thw\"] for f in flattened if \"image_grid_thw\" in f]\n",
    "\n",
    "    # Pad ids and labels using tokenizer\n",
    "    padded_inputs = processor.tokenizer.pad(\n",
    "        {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
    "        padding=\"longest\",\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    padded_labels = processor.tokenizer.pad(\n",
    "        {\"input_ids\": labels},\n",
    "        padding=\"longest\",\n",
    "        return_tensors=\"pt\",\n",
    "    ).input_ids\n",
    "\n",
    "    pad_token_id = processor.tokenizer.pad_token_id\n",
    "    padded_labels[padded_labels == pad_token_id] = -100\n",
    "\n",
    "    # Stack pixel_values\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    \n",
    "    batch = {\n",
    "        \"input_ids\": padded_inputs.input_ids,\n",
    "        \"attention_mask\": padded_inputs.attention_mask,\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"labels\": padded_labels,\n",
    "    }\n",
    "    \n",
    "    # Add image_grid_thw if present - stack along batch dimension\n",
    "    if image_grid_thw:\n",
    "        # Each element is (1, 3) or (3,) -> ensure consistent shape and stack\n",
    "        grids = []\n",
    "        for grid in image_grid_thw:\n",
    "            if grid.ndim == 1:\n",
    "                # Shape (3,) -> add batch dim -> (1, 3)\n",
    "                grid = grid.unsqueeze(0)\n",
    "            grids.append(grid)\n",
    "        # Stack to (batch_size, 3)\n",
    "        batch[\"image_grid_thw\"] = torch.cat(grids, dim=0)\n",
    "\n",
    "    return batch\n",
    "\n",
    "print(\"‚úÖ Training arguments set!\")\n",
    "print(f\"   Saving checkpoints to: {OUTPUT_DIR}\")\n",
    "print(f\"   Saving/Evaluating every {SAVE_STEPS} steps\")\n",
    "print(f\"   Gradient Checkpointing: {training_args.gradient_checkpointing}\")\n",
    "print(f\"   ‚ö° Batch={BATCH_SIZE}, GradAccum={GRADIENT_ACCUMULATION_STEPS} ‚Üí ~2-3x faster!\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache() \n",
    "    print(f\"üíæ GPU Memory before training: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ccf511-716f-40c2-9a61-c233d3b2fe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "import time\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,   # use datasets with set_transform\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=custom_collator,\n",
    "    processing_class=processor.tokenizer,  # use processing_class instead of deprecated tokenizer\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" \"*20 + \"üöÄ STARTING FINE-TUNING (2B Model)\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    print(\"üèãÔ∏è‚Äç‚ôÇÔ∏è Training...\")\n",
    "    train_result = trainer.train()\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\n‚úÖ Training complete!\")\n",
    "    print(f\"   Total Time: {elapsed/3600:.2f} hours ({elapsed/60:.1f} minutes)\")\n",
    "\n",
    "    metrics = train_result.metrics\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()\n",
    "    print(\"   Metrics saved.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Training failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "     # Clean up GPU memory\n",
    "     if torch.cuda.is_available():\n",
    "          del model\n",
    "          del trainer\n",
    "          torch.cuda.empty_cache()\n",
    "          print(f\"   GPU Memory Cleared. Final Usage: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619ff2b0-32bc-41e4-a60c-825379280710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from peft import PeftModel\n",
    "\n",
    "print(\"\\nüíæ Saving final model artifacts...\")\n",
    "print(f\"   The best model checkpoint (LoRA adapter) was saved by the trainer inside: {OUTPUT_DIR}\")\n",
    "\n",
    "# --- Save the processor ---\n",
    "try:\n",
    "    if 'processor' not in locals():\n",
    "        processor = AutoProcessor.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "    \n",
    "    best_checkpoint_path = trainer.state.best_model_checkpoint\n",
    "    if best_checkpoint_path:\n",
    "        print(f\"   Saving processor to best checkpoint: {best_checkpoint_path}\")\n",
    "        processor.save_pretrained(best_checkpoint_path)\n",
    "    else:\n",
    "        print(\"   Could not determine best checkpoint. Saving processor to main output dir.\")\n",
    "        processor.save_pretrained(str(OUTPUT_DIR))\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"   Could not save processor: {e}\")\n",
    "\n",
    "# --- Optional: Merge LoRA weights with base model ---\n",
    "merge_model = False # Set to True to try merging\n",
    "\n",
    "if merge_model:\n",
    "    print(\"\\nüîÑ Merging LoRA weights into the base model...\")\n",
    "    \n",
    "    try:\n",
    "         if torch.cuda.is_available():\n",
    "              torch.cuda.empty_cache()\n",
    "              \n",
    "         base_model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "              MODEL_NAME,\n",
    "              device_map=\"auto\",\n",
    "              torch_dtype=torch.float16, \n",
    "              trust_remote_code=True\n",
    "         )\n",
    "         \n",
    "         adapter_path = trainer.state.best_model_checkpoint\n",
    "         if adapter_path is None:\n",
    "              raise ValueError(\"Could not find best model checkpoint to merge.\")\n",
    "              \n",
    "         print(f\"   Loading adapter from: {adapter_path}\")\n",
    "         merged_model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "         \n",
    "         print(\"   Merging...\")\n",
    "         merged_model = merged_model.merge_and_unload()\n",
    "         print(\"   Merging complete.\")\n",
    "\n",
    "         MERGED_OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "         merged_model.save_pretrained(str(MERGED_OUTPUT_DIR))\n",
    "         processor.save_pretrained(str(MERGED_OUTPUT_DIR))\n",
    "         print(f\"‚úÖ Merged model saved to: {MERGED_OUTPUT_DIR}\")\n",
    "\n",
    "         del base_model\n",
    "         del merged_model\n",
    "         if torch.cuda.is_available():\n",
    "              torch.cuda.empty_cache()\n",
    "\n",
    "    except Exception as e:\n",
    "         print(f\"‚ùå Failed to merge model: {e}\")\n",
    "         print(\"   Skipping merge. You can still use the LoRA adapter from the checkpoint directory.\")\n",
    "\n",
    "print(\"\\nüéâ FINE-TUNING PROCESS COMPLETE!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CamberPy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
